{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,BatchNormalization\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making model\n",
    "model=Sequential()\n",
    "inputShape=(28,28,1)\n",
    "#Conv2D did not accept(784,-1),(784,testcases) as an input shape\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=inputShape))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10,activation='linear'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21001 samples, validate on 21000 samples\n",
      "Epoch 1/9\n",
      "21001/21001 [==============================] - 16s 745us/step - loss: 0.5689 - acc: 0.8953 - val_loss: 0.3399 - val_acc: 0.9337\n",
      "Epoch 2/9\n",
      "21001/21001 [==============================] - 15s 718us/step - loss: 0.2029 - acc: 0.9672 - val_loss: 0.2250 - val_acc: 0.9478\n",
      "Epoch 3/9\n",
      "21001/21001 [==============================] - 15s 708us/step - loss: 0.1103 - acc: 0.9802 - val_loss: 0.1153 - val_acc: 0.9720\n",
      "Epoch 4/9\n",
      "21001/21001 [==============================] - 17s 817us/step - loss: 0.0695 - acc: 0.9866 - val_loss: 0.1379 - val_acc: 0.9610\n",
      "Epoch 5/9\n",
      "21001/21001 [==============================] - 17s 825us/step - loss: 0.0449 - acc: 0.9923 - val_loss: 0.1127 - val_acc: 0.9667\n",
      "Epoch 6/9\n",
      "21001/21001 [==============================] - 16s 758us/step - loss: 0.0323 - acc: 0.9947 - val_loss: 0.0929 - val_acc: 0.9738\n",
      "Epoch 7/9\n",
      "21001/21001 [==============================] - 16s 742us/step - loss: 0.0222 - acc: 0.9971 - val_loss: 0.1482 - val_acc: 0.9530\n",
      "Epoch 8/9\n",
      "21001/21001 [==============================] - 17s 833us/step - loss: 0.0160 - acc: 0.9980 - val_loss: 0.0823 - val_acc: 0.9757\n",
      "Epoch 9/9\n",
      "21001/21001 [==============================] - 21s 999us/step - loss: 0.0120 - acc: 0.9988 - val_loss: 0.0783 - val_acc: 0.9769\n",
      "Train on 21000 samples, validate on 21001 samples\n",
      "Epoch 1/9\n",
      "21000/21000 [==============================] - 20s 952us/step - loss: 0.0801 - acc: 0.9767 - val_loss: 0.0179 - val_acc: 0.9953\n",
      "Epoch 2/9\n",
      "21000/21000 [==============================] - 20s 950us/step - loss: 0.0382 - acc: 0.9897 - val_loss: 0.0217 - val_acc: 0.9941\n",
      "Epoch 3/9\n",
      "21000/21000 [==============================] - 22s 1ms/step - loss: 0.0263 - acc: 0.9933 - val_loss: 0.0302 - val_acc: 0.9907\n",
      "Epoch 4/9\n",
      "21000/21000 [==============================] - 27s 1ms/step - loss: 0.0176 - acc: 0.9967 - val_loss: 0.0236 - val_acc: 0.9930\n",
      "Epoch 5/9\n",
      "21000/21000 [==============================] - 22s 1ms/step - loss: 0.0134 - acc: 0.9974 - val_loss: 0.0400 - val_acc: 0.9873\n",
      "Epoch 6/9\n",
      "21000/21000 [==============================] - 22s 1ms/step - loss: 0.0105 - acc: 0.9984 - val_loss: 0.0235 - val_acc: 0.9927\n",
      "Epoch 7/9\n",
      "21000/21000 [==============================] - 22s 1ms/step - loss: 0.0072 - acc: 0.9992 - val_loss: 0.0335 - val_acc: 0.9894\n",
      "Epoch 8/9\n",
      "21000/21000 [==============================] - 26s 1ms/step - loss: 0.0056 - acc: 0.9995 - val_loss: 0.0207 - val_acc: 0.9931\n",
      "Epoch 9/9\n",
      "21000/21000 [==============================] - 23s 1ms/step - loss: 0.0045 - acc: 0.9998 - val_loss: 0.0226 - val_acc: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0252d36be0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making dataloader,\n",
    "# this was mainly to convert pandas dataframe to numpy array\n",
    "# keras inbuilt functionality and reshape functionality are\n",
    "# available for numpy.\n",
    "\n",
    "import pandas as pd\n",
    "def MnistLoadData(X,Y,frac):\n",
    "    lenx=len(X)\n",
    "    lenY=len(Y)\n",
    "    #splitting input data to test and train\n",
    "    x1=X.loc[0:lenx*frac]\n",
    "    x2=X.loc[lenx*(1-frac):lenx]\n",
    "    y1=Y.loc[0:lenY*frac]\n",
    "    y2=Y.loc[lenY*(1-frac):lenY]\n",
    "    #converting pandas datafram to numpy\n",
    "    #for performing reshape and using \n",
    "    #to_categorical functions of keras\n",
    "    x1=x1.to_numpy()\n",
    "    x2=x2.to_numpy()\n",
    "    y1=y1.to_numpy()\n",
    "    y2=y2.to_numpy()\n",
    "    x1=x1.reshape(x1.shape[0],28,28,1)\n",
    "    x2=x2.reshape(x2.shape[0],28,28,1)\n",
    "    #one hot encoding functionality of keras\n",
    "    #converts y to its corresponding probablity array\n",
    "    #1=[1,0,0,0,0,0,0,0...]\n",
    "    #2=[0,1,0,0,0,0,0,....]\n",
    "    y1=keras.utils.to_categorical(y1,10)\n",
    "    y2=keras.utils.to_categorical(y2,10)\n",
    "    return (x1,y1),(x2,y2)\n",
    "\n",
    "#reading dataset\n",
    "dataset=pd.read_csv('dataset/digit-recognizer/train.csv')\n",
    "X=dataset.drop('label',axis=1)\n",
    "Y=dataset['label']\n",
    "\n",
    "#loading data\n",
    "(x1,y1),(x2,y2)=MnistLoadData(X,Y,.5)\n",
    "#making model, here neural net extracts characteristic weights for this dataset\n",
    "#so that it can make predictions\n",
    "model.fit(x1,y1,batch_size=128,epochs=9,verbose=1,validation_data=(x2,y2))#epoch is set at 9 because that is where inflection happens in acc\n",
    "model.fit(x2,y2,batch_size=128,epochs=9,verbose=1,validation_data=(x1,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000/21000 [==============================] - 4s 208us/step\n",
      "[0.002288679586695729, 0.9998571428571429]\n"
     ]
    }
   ],
   "source": [
    "#check using the second half of the dataset, how accurate the prediction is\n",
    "acc=model.evaluate(x2,y2,verbose=1)\n",
    "print(acc)\n",
    "#loading data from csv for predictions\n",
    "#xtest need not be seperated since it only has input vector\n",
    "Xtest=pd.read_csv('dataset/digit-recognizer/test.csv')\n",
    "Xtest=Xtest.to_numpy()\n",
    "Xtest=Xtest.reshape(Xtest.shape[0],28,28,1)\n",
    "Ytest=model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the submission file\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "Yarray=[]\n",
    "i=1\n",
    "Yarray.append((\"ImageId\",\"Label\"))\n",
    "for nu in Ytest:\n",
    "    Yarray.append((i,int(argmax(nu))))\n",
    "    i=i+1\n",
    "\n",
    "np.savetxt(\"dataset/digit-recognizer/submission.csv\",np.asarray(Yarray),delimiter=\",\",fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
