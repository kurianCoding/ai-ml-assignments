{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making model\n",
    "model=Sequential()\n",
    "inputShape=(28,28,1)\n",
    "#Conv2D did not accept(784,-1),(784,testcases) as an input shape\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=inputShape))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=inputShape))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21001 samples, validate on 21000 samples\n",
      "Epoch 1/7\n",
      "21001/21001 [==============================] - 29s 1ms/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.0595 - val_acc: 0.9851\n",
      "Epoch 2/7\n",
      "21001/21001 [==============================] - 33s 2ms/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.1005 - val_acc: 0.9761\n",
      "Epoch 3/7\n",
      "21001/21001 [==============================] - 34s 2ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0676 - val_acc: 0.9833\n",
      "Epoch 4/7\n",
      "21001/21001 [==============================] - 36s 2ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0721 - val_acc: 0.9825\n",
      "Epoch 5/7\n",
      "21001/21001 [==============================] - 34s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 0.9838\n",
      "Epoch 6/7\n",
      "21001/21001 [==============================] - 34s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0619 - val_acc: 0.9844\n",
      "Epoch 7/7\n",
      "21001/21001 [==============================] - 30s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fca503f49e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making dataloader,\n",
    "# this was mainly to convert pandas dataframe to numpy array\n",
    "# keras inbuilt functionality and reshape functionality are\n",
    "# available for numpy.\n",
    "\n",
    "import pandas as pd\n",
    "def MnistLoadData(X,Y):\n",
    "    lenx=len(X)\n",
    "    lenY=len(Y)\n",
    "    #splitting input data to test and train\n",
    "    x1=X.loc[0:lenx/2]\n",
    "    x2=X.loc[lenx/2:lenx]\n",
    "    y1=Y.loc[0:lenY/2]\n",
    "    y2=Y.loc[lenY/2:lenY]\n",
    "    #converting pandas datafram to numpy\n",
    "    #for performing reshape and using \n",
    "    #to_categorical functions of keras\n",
    "    x1=x1.to_numpy()\n",
    "    x2=x2.to_numpy()\n",
    "    y1=y1.to_numpy()\n",
    "    y2=y2.to_numpy()\n",
    "    x1=x1.reshape(x1.shape[0],28,28,1)\n",
    "    x2=x2.reshape(x2.shape[0],28,28,1)\n",
    "    #one hot encoding functionality of keras\n",
    "    #converts y to its corresponding probablity array\n",
    "    #1=[1,0,0,0,0,0,0,0...]\n",
    "    #2=[0,1,0,0,0,0,0,....]\n",
    "    y1=keras.utils.to_categorical(y1,10)\n",
    "    y2=keras.utils.to_categorical(y2,10)\n",
    "    return (x1,y1),(x2,y2)\n",
    "\n",
    "#reading dataset\n",
    "dataset=pd.read_csv('dataset/digit-recognizer/train.csv')\n",
    "X=dataset.drop('label',axis=1)\n",
    "Y=dataset['label']\n",
    "\n",
    "#loading data\n",
    "(x1,y1),(x2,y2)=MnistLoadData(X,Y)\n",
    "#making model, here neural net extracts characteristic weights for this dataset\n",
    "#so that it can make predictions\n",
    "model.fit(x1,y1,batch_size=128,epochs=7,verbose=1,validation_data=(x2,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000/21000 [==============================] - 7s 333us/step\n",
      "[0.060601188722526005, 0.9850476190476191]\n"
     ]
    }
   ],
   "source": [
    "#check using the second half of the dataset, how accurate the prediction is\n",
    "acc=model.evaluate(x2,y2,verbose=1)\n",
    "print(acc)\n",
    "#loading data from csv for predictions\n",
    "#xtest need not be seperated since it only has input vector\n",
    "Xtest=pd.read_csv('dataset/digit-recognizer/test.csv')\n",
    "Xtest=Xtest.to_numpy()\n",
    "Xtest=Xtest.reshape(Xtest.shape[0],28,28,1)\n",
    "Ytest=model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the submission file\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "Yarray=[]\n",
    "i=1\n",
    "for nu in Ytest:\n",
    "    Yarray.append((i,int(argmax(nu))))\n",
    "    i=i+1\n",
    "\n",
    "np.savetxt(\"dataset/digit-recognizer/submission.csv\",np.asarray(Yarray),delimiter=\",\",fmt=\"%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
